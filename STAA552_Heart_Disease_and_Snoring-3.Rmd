---
output:
  pdf_document: default
  html_document: default
  word_document: default
---
STAA552 Handout: Heart Disease and Snoring
========================================================
#### Jay Breidt, Department of Statistics, Colorado State University

It will be convenient for this example to enter the response variable as a two-column matrix with the columns giving the numbers of successes and failures, which is what the following code does:  
```{r}
heartDisease <-
  array(c(24, 35, 21, 30, 1355, 603, 192, 224),
  dim = c(4, 2),
  dimnames = list(
  Snoring = c("Never", "Occasional", "Almost Always", "Always"),
  HeartDisease = c("Yes", "No")
  )
  )
Snoring_Score <- c(0, 2, 4, 5)
heartDisease
```
In this case, we have $N=4$ binomial observations, with different sample sizes $n_i$ for each.  


## Saturated model

The saturated model gives
$$
\hat\theta_i=\frac{\mbox{number Yes}}{\mbox{number Yes}+\mbox{number No}}
$$
for $i=1,2,3,4$.

Compute the estimated probabilities for this saturated model and the
corresponding empirical logits and plot the logits versus snoring score:
```{r}
# Compute n_i
n_i <- apply(heartDisease, MAR = 1, FUN = "sum") #compute row sums
Empirical_Prob <- heartDisease[, 1] / n_i
Empirical_Logit <- log(Empirical_Prob / (1 - Empirical_Prob))
plot(Snoring_Score, Empirical_Logit, pch = 16, col = "blue")
```

The saturated model has four parameters.  We can fit the saturated model and see that it fits perfectly:
```{r}
sat <- glm(heartDisease ~ factor(Snoring_Score), family = binomial(link = logit))
summary(sat)
```
Fisher Scoring is the iterative numerical optimization method used to get the
parameter estimates.

Notice that the saturated model reproduces the simple proportions:
```{r}
predict.glm(sat, type = "response")
Empirical_Prob
```

Another way to fit a saturated model would be to treat `Snoring_Score` as a continuous variable and use a **cubic polynomial** (remember that an intercept is included by default):
```{r}
cubic <- glm(heartDisease ~ poly(Snoring_Score, 3), family = binomial(link = logit))
summary(cubic)
predict(cubic, type = "response")
Empirical_Prob
```
Why does this fit perfectly?

**Note:** the `poly` function in `R` does not just create $x, x^2, \ldots,x^p$, but actually creates **orthogonal** polynomials.  Hence, `poly(Snoring_Score,3)3` is not simply the coefficient of `Snoring_Score ^ 3`.


## Quadratic model

Now let's fit a simpler model than the saturated model.  How about a **quadratic polynomial** on the logit scale?

The formula and family statements specify that the response  `heartDisease` will be modeled as a binomial random variable with mean that is a function of `Snoring_Score` and an intercept, included by default.

```{r}
quad <- glm(heartDisease ~ poly(Snoring_Score, 2), family = binomial(link = logit))
summary(quad)
```
The `Null deviance` is for a model with **only** the intercept.  The `Residual deviance` is for our model with $p+1=3$ parameters for the intercept, linear, and quadratic terms.  Because we have grouped data, we can conduct the formal lack-of-fit test, comparing our quadratic model to the saturated model.  The null hypothesis is that the quadratic model fits adequately and the alternative is that it does not (it is oversimplified).  We conduct the test by comparing `Residual deviance`
to $\chi^2_{4-3}=\chi^2_1$, 
```{r}
quad$deviance
pchisq(quad$deviance,df = 1,lower.tail=F)
```
for which the p-value indicates no evidence of lack-of-fit. 

But can we simplify further?

## Linear model

Now let's fit an even simpler model: **linear** instead of quadratic: 
```{r}
line <- glm(heartDisease ~ Snoring_Score, family = binomial(link = logit))
summary(line)
pchisq(line$deviance, df = 2, lower.tail = F)
```
Once again, the `Null deviance` is for a model with **only** the intercept.  The `Residual deviance` is for our model with $p+1=2$ parameters, $\beta_0$ and $\beta_1$, that show up in our probabilities as 
$$
\theta_i=\frac{\exp(\beta_0+\beta_1\mbox{Snoring$\_$Score}_i)}{1+\exp(\beta_0+\beta_1\mbox{Snoring$\_$Score}_i)}
$$
with estimated values $\widehat\beta_0= `r line$coef[1]`$ and $\widehat\beta_1= `r line$coef[2]`$.  


If we had started with the linear specification, we could test its lack-of-fit relative to the saturated model, using  $\chi^2_{4-2}=\chi^2_2$, 
```{r}
pchisq(line$deviance,df = 2,lower.tail=F)
```
for which the p-value indicates no evidence of lack-of-fit. 

Alternatively, if we had started with the quadratic specification, we could do a drop-in-deviance test to see if the linear model fits as well as the quadratic model (already shown to fit): 
```{r}
# Drop-in-deviance test between linear and quadratic model.
line$deviance - quad$deviance
pchisq(line$deviance - quad$deviance, df = 1, lower = F)
```
This is a bit borderline, but we'd definitely prefer a simpler model for such a small data set (and might not want to get too carried away with the numerical properties of the snoring score).

Next, let's add the fitted logits for our linear specification to our plot of empirical logits:
```{r}
Predicted_Logit <- predict.glm(line, type = "link")
plot(c(Snoring_Score, Snoring_Score), c(Empirical_Logit, Predicted_Logit), 
     type = "n", ylab = "Logit", xlab = "Snoring Score")
points(Snoring_Score, Empirical_Logit, col = "blue", pch = 16)
points(Snoring_Score, Predicted_Logit, col = "red", pch = 17)
```

Fitted values (triangles) track the empirical logits (dots) quite well.


## Null model

One last test: could we drop snoring scores from the model and use only the overall intercept? That is, are the odds of heart disease (and hence probability of heart disease) constant across snoring levels?


```{r}
# Drop-in-deviance test between null and linear model.
line$null.deviance - line$deviance
pchisq(line$null.deviance - line$deviance, df = 1, lower = F)
```

Definitely not!